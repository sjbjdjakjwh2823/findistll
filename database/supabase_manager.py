import os
import logging
import pandas as pd
from supabase import create_client, Client
import psycopg2
from psycopg2 import sql

logger = logging.getLogger(__name__)

class SupabaseManager:
    def __init__(self):
        from dotenv import load_dotenv
        load_dotenv()
        self.url = os.getenv("SUPABASE_URL")
        self.key = os.getenv("SUPABASE_KEY")
        self.db_connection_string = os.getenv("SUPABASE_DB_URL") # postgresql://...
        
        self.client: Client = None
        if self.url and self.key:
            self.client = create_client(self.url, self.key)
            self._ensure_tables()
        else:
            logger.warning("Supabase credentials missing. Running in OFF-LINE mode.")

    def _ensure_tables(self):
        """
        Auto-create tables if they don't exist.
        Requires 'SUPABASE_DB_URL' (Postgres connection string) for DDL execution.
        If only API key is available, we assume tables exist or warn.
        """
        if not self.db_connection_string:
            logger.warning("SUPABASE_DB_URL not set. Skipping table creation checks (DDL requires direct DB access).")
            return

        try:
            conn = psycopg2.connect(self.db_connection_string)
            conn.autocommit = True
            cursor = conn.cursor()
            
            # 1. market_data
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS market_data (
                    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
                    entity VARCHAR NOT NULL,
                    period TIMESTAMP WITH TIME ZONE NOT NULL,
                    concept VARCHAR NOT NULL,
                    value DOUBLE PRECISION,
                    unit VARCHAR,
                    source_tier VARCHAR,
                    ingested_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                    UNIQUE (entity, period, concept)
                );
            """)
            
            # 2. ai_training_sets (Spoke A)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS ai_training_sets (
                    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
                    instruction TEXT,
                    input TEXT,
                    output TEXT,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
            """)
            
            # 3. geopolitical_events
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS geopolitical_events (
                    event_id VARCHAR PRIMARY KEY,
                    headline TEXT,
                    source VARCHAR,
                    period TIMESTAMP WITH TIME ZONE,
                    related_entities TEXT[],
                    impact_score DOUBLE PRECISION,
                    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                );
            """)
            
            # 4. profiles (User management)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS profiles (
                    id UUID REFERENCES auth.users ON DELETE CASCADE,
                    email VARCHAR,
                    subscription_tier VARCHAR DEFAULT 'Free',
                    api_calls_remaining INTEGER DEFAULT 100,
                    PRIMARY KEY (id)
                );
            """)
            
            # Enable RLS (Security)
            tables = ['market_data', 'ai_training_sets', 'geopolitical_events', 'profiles']
            for t in tables:
                cursor.execute(sql.SQL("ALTER TABLE IF EXISTS {} ENABLE ROW LEVEL SECURITY;").format(sql.Identifier(t)))
                
            conn.close()
            logger.info("Supabase tables verified/created successfully.")
            
        except Exception as e:
            logger.error(f"Supabase Table Init Failed: {e}")

    def upsert_market_data(self, data_list):
        """
        Bulk upsert to 'market_data'
        Handles Wide Format (OHLCV) -> Long Format (Concept/Value) conversion.
        """
        if not self.client or not data_list: return
        
        # Convert to DataFrame for easier manipulation
        df = pd.DataFrame(data_list)
        
        # Replace NaN with None (which becomes JSON null)
        df = df.where(pd.notnull(df), None)
        
        # Date Standardization
        if 'period' in df.columns:
            # dt.isoformat() is not available in pandas Series dt accessor directly in older versions?
            # Use apply(lambda x: x.isoformat()) or dt.strftime
            # But ISO format is best handled by to_datetime and then strftime
            df['period'] = pd.to_datetime(df['period'], format='mixed', errors='coerce', utc=True).dt.strftime('%Y-%m-%dT%H:%M:%S%z')
            
        # Filter out columns that are NOT in schema (e.g. meta, date, high, low if already melted)
        # We need to do this carefully. 
        # If we melt, we filter AFTER melting.
        # If we DON'T melt (already Long), we should filter HERE.
        
        # Define Schema Columns
        schema_cols = ['entity', 'period', 'concept', 'value', 'unit', 'source_tier', 'ingested_at']

        # Wide-to-Long Transformation
        # Check if we have OHLCV columns
        wide_cols = ['open', 'high', 'low', 'close', 'volume']
        if any(c in df.columns for c in wide_cols):
             # Identify id_vars (everything that is NOT a metric)
             id_vars = [c for c in df.columns if c not in wide_cols and c != 'value' and c != 'concept']
             
             # Melt
             df_long = df.melt(id_vars=id_vars, value_vars=[c for c in wide_cols if c in df.columns], 
                               var_name='concept', value_name='value')
             
             # Standardize Concept Names
             concept_map = {
                 'close': 'MarketClose',
                 'open': 'MarketOpen',
                 'high': 'MarketHigh',
                 'low': 'MarketLow',
                 'volume': 'MarketVolume'
             }
             df_long['concept'] = df_long['concept'].map(lambda x: concept_map.get(x, x))
             
             # Drop rows with NaN values
             df_long = df_long.dropna(subset=['value'])
             
             # Filter out undefined columns (keep only what's in schema)
             valid_cols = ['entity', 'period', 'concept', 'value', 'unit', 'source_tier', 'ingested_at']
             df_long = df_long[[c for c in df_long.columns if c in valid_cols]]
             
             # Convert back to list of dicts
             data_list = df_long.to_dict(orient='records')
        else:
             # Already Long Format (or unknown), just filter columns
             df_filtered = df[[c for c in df.columns if c in schema_cols]]
             data_list = df_filtered.to_dict(orient='records')
        
        # Supabase API 'upsert'
        # Chunking for bulk request
        chunk_size = 500
        for i in range(0, len(data_list), chunk_size):
            chunk = data_list[i:i + chunk_size]
            try:
                self.client.table("market_data").upsert(chunk, on_conflict="entity, period, concept").execute()
            except Exception as e:
                logger.error(f"Supabase Upsert Error (Batch {i//chunk_size}): {e}")
        
        logger.info(f"Supabase: Upsert process complete for {len(data_list)} records.")

    def upsert_ai_training_data(self, data_list):
        if not self.client or not data_list: return
        try:
            # JSONL list to Table
            self.client.table("ai_training_sets").insert(data_list).execute()
            logger.info(f"Supabase: Inserted {len(data_list)} AI training records.")
        except Exception as e:
            logger.error(f"Supabase AI Data Error: {e}")

    def upsert_geopolitical_events(self, data_list):
        """
        Bulk upsert to 'geopolitical_events'
        """
        if not self.client or not data_list: return
        
        # Convert to DataFrame to handle NaN/None
        df = pd.DataFrame(data_list)
        df = df.where(pd.notnull(df), None)
        data_list = df.to_dict(orient='records')

        try:
            self.client.table("geopolitical_events").upsert(data_list).execute()
            logger.info(f"Supabase: Inserted {len(data_list)} geopolitical events.")
        except Exception as e:
            logger.error(f"Supabase Geopolitical Upsert Error: {e}")

    def close(self):
        pass # Supabase client handles connection pooling internally
